{
 "cells": [
  {
   "cell_type": "raw",
   "id": "63df7c28-ab5a-48eb-aeeb-7f063a67960d",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "sol:\n",
    "The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning method employed to tackle classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0635c0d5-f67d-44b0-a05f-68cddafd6f9f",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?\n",
    "Kvalue indicates the count of the nearest neighbors. We have to compute distances between test points and trained labels points. Updating distance metrics with every iteration is computationally expensive, and that’s why KNN is a lazy learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c177e-6368-488a-acd5-f5bdeb7c0571",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "The key differences are: KNN regression tries to predict the value of the output variable by using a local average. KNN classification attempts to predict the class to which the output variable belong by computing the local probability. Knn Classifier: Predicts a by using the category among its k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97dbf9-2f2f-4425-b750-d1b1db0ca131",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?\n",
    "we know the actual category of observations in the test dataset, the performance of the kNN model can be evaluated. One of the most commonly used parameter is the average accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab5189-a39d-45e5-8558-05623b2488c1",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n",
    "The Curse of Dimensionality refers to various phenomena that arise when dealing with high-dimensional data. As the number of features or dimensions increases, the volume of the feature space grows exponentially, leading to sparsity in the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81faeca9-e342-4a8f-8a23-dd8b94b30446",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?\n",
    "K-Nearest Neighbors Imputation: K-Nearest Neighbors (KNN) is a machine learning algorithm that can be used for imputing missing values by finding the K nearest neighbors of the instance with the missing value and filling in the missing value with the average of these neighbors. We can use the KNNImputer class from the pyspark.ml.feature module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f31f6b-103f-4027-b9b5-95df87b38ffe",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "KNN regression tries to predict the value of the output variable by using a local average.\n",
    "KNN classification attempts to predict the class to which the output variable belong by computing the local probability."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5716a047-9588-44c9-9a56-fc5330389d04",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "Advantages of KNN\n",
    "1. No Training Period: KNN is called Lazy Learner (Instance based learning). It does not learn anything in the training period. It does not derive any discriminative function from the training data. In other words, there is no training period for it. It stores the training dataset and learns from it only at the time of making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g. SVM, Linear Regression etc.\n",
    "\n",
    "2. Since the KNN algorithm requires no training before making predictions, new data can be added seamlessly which will not impact the accuracy of the algorithm.\n",
    "\n",
    "3. KNN is very easy to implement. There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)\n",
    "\n",
    "Disadvantages of KNN\n",
    "1. Does not work well with large dataset: In large datasets, the cost of calculating the distance between the new point and each existing point is huge which degrades the performance of the algorithm.\n",
    "\n",
    "2. Does not work well with high dimensions: The KNN algorithm doesn’t work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate the distance in each dimension.\n",
    "\n",
    "3. Need feature scaling: We need to do feature scaling (standardization and normalization) before applying KNN algorithm to any dataset. If we don’t do so, KNN may generate wrong predictions.\n",
    "\n",
    "4. KNN is sensitive to noise in the dataset. We need to manually impute missing values and remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ff4055-75f3-4993-bf28-0c27068918c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
